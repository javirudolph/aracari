---
title: "Variation in distance between consecutive animal locations"
abstract: |
  This should be the abstract
output:
  word_document:
    reference_docx: docx_ref.docx
  html_document:
    df_print: paged
bibliography: aracari_refs.bib
csl: ecology-letters.csl
---

# Introduction
   Movement is a fundamental ecological process, critical to an animal's biology and it's interactions with the environment. The study of animal movement tends to focus on four basic mechanistic questions: the why, when, where, and how an animal moves, and these questions can be answered at the scales of individuals or populations. A general framework for movement ecology that allows to link movement patterns and processes, starts with a focus on movement itself and the individual's internal state, navigation capacity, motion capacity, and responses to external factors [@nathan2008movement]. The overall goal of studying movement seeks to find the linkages between the causes and mechanisms of animal movement, that result in spatiotemporal patterns that feed back into various ecological and evolutionary processes. Some of these approaches involve separating different movement patterns into behavioral states, with each state having an associated set of parameters that determine that specific movement pattern[@langrock2012flexible]. 
 
  Although there is variation in animal movement at the individual scale, the general focus has been on understanding the typical movement patterns, built around average measures at population levels, with the simplest approach assuming model parameters are the same across individuals, known as 'complete pooling' [@langrock2012flexible]. This focus on generality has driven to a lack of frameworks for understanding variation, how it is maintained, and the consequences of this variation across populations, communities, and ecosystems. Some alternatives have been considered to incorporate individual variation with 'no pooling' or 'partial pooling' approaches, where each or some individuals have their own set of parameters [@patterson2009classifying, @jonsen2006robust], however a high number of parameters quickly limits applications to high levels of individual variation. Recent calls for incorporating individual variation in movement [@snell2019consequences, @shaw2020causes] have pushed for the need to develop theory that explores different types of variation in movement patterns to understand its consequences across ecological organizational levels. In particular, when considering species interactions, incorporating individual variation becomes essential to understand how positive or negative feedback loops play into the maintenance of variation in movement as individuals respond to external factors in their environment [@shaw2020causes]. 
  
  
  
**Could I find anything pre HMMs of SSMs? Wondering what people could do before? And wondering this because we have broken up tracking periods, not continuous ones**  
Seems like I need to focus more on random walks
  
Assume weibull distributions for step length and wrapped cauchy distributions for the turning angles [@langrock2012flexible], but they tried Gamma and von Mises, respectively, but got outperformed by AIC. Used negative binomial for the state dwell times. 
  
Random walks used broadly in biology to characterize animal and cell movement. Describes que commonly used distributions for angles: von Mises, wrapped normal and wrapped Cauchy distributions. [@codling2008random]
- IDEA: so maybe use these distributions for the angles. We might see that there is not much difference in angle movement across individuals? They all tend to move forward perhaps, or something like that, and where the real differences are is in the distances. 
- Also might be intriguing to incorporate the time as a set of multiple random walks? or repeats the velocity value for that number of minutes. Maybe just using a correlated random walk, which takes into account short-term correlations in the direction of movement?
  - Codling describes this as a velocity jump process, since the markov process is the walker's velocity rather than the location. Details at the end of page 7. 
  - We consider the individuals moving in space at a constant speed $\nu$, and at each time step $\tau$ each individual changes direction and moves a distance $\delta$ in a new direction (with probability $r=\lambda\tau$), or moves a distance $\delta$ in the previous direction (with probability $q=1-\lambda\tau$). Hence, turning events occur as a Poisson process with rate $\lambda$. **This is textual from paper, so modify** Their focus is also on multiple individuals at the same time, and the density of individuals at a given time and location.
    - In my case, the distance $\delta$ would also be randomly sampled, or this are the velocities I suppose. So, perhaps it is constant speed, and the distance is actually times to account for 15, 30, etc minutes based on the data. 
  


As in other papers. Note that we adopt the convention of using upper case letters for random variables and lower case letters for possible numerical values of these random variables. 
  
# Methods
Describe the data collection from the animov outline. 
Add a section that describes the limitations for this telemetry data. 
From book chapter: Attached radio-transmitters at the base of the central tail feathers. Toucan locations measured by triangulation using receivers and hand-held two-element Yagi antennas. 
Tracking periods laster 4-5 hours with individual birds located every 15 minutes. The 15 minute interval to ensure locations within the time frame of seed passage or regurgitation, which are expected to be longer than 15 minutes.

"Location accuracy using radio telemetry may be reduced in tropical forests... conservative distance categories of 100-m increments to beteter represent precision. Time categories: 15, 30, 60, 90 minutes. Then calculated probability of movements made within each distance category within each time category, summed accross each time category to give a final probability for each distance category."

"Distances travelled per movement bout ranged from 0 to >2000m. Strong leptokurtic distributions of movements with most being <300m. Longest recorded movement for Pluricinctus is 3665m (recorded in 30 minutes)

From each of the locations for the twelve individuals, we calculated turning angles (in radians) between subsequent directions, and the associated Euclidean step lengths in meters.

Evaluating models with their individual-specific counterparts, using AIC or BIC. In individual-specific models the AIC results from the joint likelihood of the individual-specific models.     

Consider incorporating NAs for missed observations so we keep the regular 15 minute time interval.

```{r setup, include=FALSE}
# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r}
library(tidyverse)
library(aracari)
library(stringr)
library(fitdistrplus)
library(cowplot)
library(RColorBrewer)
library(gt)

theme_set(theme_bw())
```

```{r}
aracari_df <- readRDS("aracari_df.RDS")
```

```{r eval=FALSE}
# Which individuals will we conside? Need at least a certain number of data points to fit distributions
# Don't remove any yet, just make sure you make it clear how many data points you have for each

aracari_df %>% 
  dplyr::filter(dist != 0) %>% 
  group_by(id) %>% 
  tally()

```

```{r}
# Create my focus variable and only for individuals with 30+ observations

# focus <- "dist"
# focus <- "R2n"
 focus <- "mpm"
# focus <- "m15"

if(focus == "dist"){
  df <- aracari_df %>%
  dplyr::filter(dist != 0) %>%
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  dplyr::filter(dist != 0) %>% 
    mutate(Bird_ID = paste0("B", id),
           focus = dist)
  
  dist.used <- c("lnorm", "weibull", "cauchy")
  param <- c("meanlog", "sdlog", "shape", "scale", "location", "scale")
  dist <- c("lnorm", "lnorm", "weibull", "weibull", "cauchy", "cauchy")
  kpars <- c(2, 2, 2, 2, 2, 2)
}

if(focus == "R2n"){
  df <- aracari_df %>%
  dplyr::filter(R2n != 0) %>%
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  dplyr::filter(R2n != 0) %>% 
    mutate(Bird_ID = paste0("B", id),
           focus = sqrt(R2n))
  
  dist.used <- c( "exp", "gamma", "weibull", "lnorm")
  param <- c("rate", "shape", "rate", "shape", "scale", "meanlog", "sdlog")
  dist <- c("exp", "gamma", "gamma", "weibull", "weibull", "lnorm", "lnorm")
  kpars <- c(1, 2, 2, 2, 2, 2, 2)
}

if(focus == "mpm"){
  df <- aracari_df %>% 
  dplyr::filter(dist != 0) %>% 
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  mutate(tmin = dt/60,
         mpm = dist/tmin,
         Bird_ID = paste0("B", id),
         focus = mpm) %>% 
  dplyr::filter(., mpm != 0)
  
  dist.used <- c( "exp", "gamma", "weibull", "lnorm")
  param <- c("rate", "shape", "rate", "shape", "scale", "meanlog", "sdlog")
  dist <- c("exp", "gamma", "gamma", "weibull", "weibull", "lnorm", "lnorm")
  kpars <- c(1, 2, 2, 2, 2, 2, 2)
}

if(focus == "m15"){
  df <- aracari_df %>% 
  dplyr::filter(dist != 0) %>% 
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  mutate(tmin = dt/60,
         mpm = dist/tmin,
         m15 = mpm*15,
         Bird_ID = paste0("B", id),
         focus = m15) %>% 
  dplyr::filter(., m15 != 0)
  
  dist.used <- c( "exp", "gamma", "weibull", "lnorm")
  param <- c("rate", "shape", "rate", "shape", "scale", "meanlog", "sdlog")
  dist <- c("exp", "gamma", "gamma", "weibull", "weibull", "lnorm", "lnorm")
  kpars <- c(1, 2, 2, 2, 2, 2, 2)
}


```

\newpage
# Questions  
* What is the distribution of distances between consecutive recorded locations? 
* Are there differences between individuals or can we use the same distribution to describe these distances between two locations?

The reasoning behind this, is that distances between locations can be used later on to describe variation in step length when simulating animal movement under simple models such as random walk.

### MPM: meters per minute
Due to the variation in time between consecutive locations, we scaled the step lengths to displacement in one minute.
**Visualize the variation and distribution of these distances between locations**  

```{r}
 df %>%
  ggplot(., aes((x = focus))) +
  geom_line(aes(x = focus, group = id), stat = "density", alpha = 0.4) +
  geom_line(stat = "density", color = "#1191d1", lwd = 1) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  labs(y = "Density", x = paste(focus)) +
  theme_classic()


```

```{r}
df %>%
  ggplot(., aes(x = focus)) +
  facet_wrap(~id) +
  geom_line(stat = "density") +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  labs(y = "Density", x = paste(focus)) +
  theme_classic()
```


# Fit distribution  
Useful resource to use the package [fitdistrplus](https://cran.r-project.org/web/packages/fitdistrplus/vignettes/FAQ.html)
Also check [this](https://stackoverflow.com/questions/37152482/power-law-fitted-by-fitdistr-function-in-package-fitdistrplus). Should I do the opimization by hand?

```{r}

# Fit the four models for the population level data
  
pop <- lapply(dist.used, function(x){fitdist(df$focus, distr = x)})

# Get data for qqplot at population level
qqpop <- qqcomp(pop, plotstyle = "ggplot")$data
names(qqpop) <- c("theoretical", "fdist", "empirical")

# Save the population level parameters

build_fits_df <- function(x){

  nm <- deparse(substitute(x))

  x %>%
    map(., `[`, c("estimate", "sd", "loglik", "n")) %>%
    map(as_tibble) %>%
    bind_rows()
}

prms.df <- build_fits_df(pop) %>% 
  mutate(param = param,
         dist = dist,
         kpars = kpars,
         data = "pop")

# Repeating the process for data at the individual level
inds <- unique(as.character(df$Bird_ID))

# Fitting the four distributions for each individual and saving the parameters in a dataframe
ind.models <- NULL
for(i in 1:length(inds)){
  fit.df <- df %>% 
    dplyr::filter(., Bird_ID == inds[i])
  
  fit <- lapply(dist.used, function(x){fitdist(fit.df$focus, distr = x)})
  
  ind.models[[i]] <- fit
  
  assign(paste(inds[i]), fit)
  
  fit.prms <- build_fits_df(fit) %>% 
  mutate(param = param,
         dist = dist,
         kpars = kpars,
         data = inds[i])
  
  prms.df <- bind_rows(prms.df, fit.prms)
}


# Getting qqplot data for each individual
inds.models.data <- NULL
for(i in 1:length(inds)){
  x <- ind.models[[i]]
  
  qqdata <- qqcomp(x, plotstyle = "ggplot")$data %>% 
    mutate(data = inds[i])
  names(qqdata) <- c("theoretical", "fdist", "empirical", "individual")
  inds.models.data <- bind_rows(inds.models.data, qqdata)
}
```


```{r ind_QQplots, fig.width=12, fig.height=5}

qqpop %>% 
  mutate(individual = "POPULATION") %>% 
  ggplot(., aes(x = theoretical, y = empirical, color = fdist)) +
  facet_wrap(~individual) +
  #coord_equal() +
  geom_point(shape = 16, alpha = 0.8, size = 4) +
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "Model Quantiles",
         y = "Empirical Quantiles") +
  #scale_color_viridis_d() +
  scale_color_brewer(palette = "Spectral", name = "Distribution") +
  theme(legend.position = c(0.8, 0.2)) +
  NULL -> a

inds.models.data%>% 
  ggplot(., aes(x = theoretical, y = empirical, color = fdist)) +
  #facet_grid(individual~fdist) +
  facet_wrap(~individual)+
    geom_point(shape = 16, alpha = 0.8, size = 3) +
    geom_abline(intercept = 0, slope = 1) +
    labs(x = "Model Quantiles",
         y = "Empirical Quantiles") +
  #scale_color_viridis_d() +
  scale_color_brewer(palette = "Spectral") +
  theme(legend.position = "none") +
  guides(color = guide_legend(title = "Distribution")) +
  #coord_equal() +
  NULL -> b

plot_grid(a,b, rel_widths = c(1,2), labels = "AUTO")

```

## Information criteria

```{r}

ic_calc <- function(n, kpars, loglik){
  
  BIC <- log(n)*kpars - 2*loglik
  AIC <- 2*kpars - 2*loglik
  AICc <- AIC + (2* kpars * (kpars+1)/(n-kpars-1))
}


prms.df %>% 
  mutate(param = str_to_upper(param), 
         dist = str_replace(dist, "exp", "Exponential"),
         dist = str_replace(dist, "gamma", "Gamma"),
         dist = str_replace(dist, "weibull", "Weibull"),
         dist = str_replace(dist, "lnorm", "Lognormal"),
         data = str_replace(data, "pop", "POP")) %>% 
  mutate(BIC = log(n)*kpars - 2*loglik,
         AIC = 2*kpars - 2*loglik,
         AICc = AIC + (2* kpars * (kpars+1)/(n-kpars-1))) %>% 
  group_by(data) %>% 
  mutate(deltaBIC = signif(BIC - min(BIC), 3),
         deltaAIC = signif(AIC - min(AIC), 3),
         deltaAICc = signif(AICc - min(AICc), 3)) %>% 
  mutate(best_bic = ifelse(deltaBIC == 0, "#ffaa00", "black"),
         best_aic = ifelse(deltaAIC == 0, "#fc0362", "black")) -> nice_prms
```

So, for each individual, which is the best fitting distribution, based on AIC or BIC? This is the distribution with the lowest AIC or BIC score for each individual. These are basically the distributions that will be used for the mixed distribution model later on. 

```{r}
nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(dist, data, deltaAIC,
                #deltaAICc,
                deltaBIC) %>% 
  distinct() %>% 
  pivot_longer(., cols = starts_with("delta"), names_to = "ic", values_to = "value") %>% 
  filter(value == 0) %>%  
  pivot_wider(., names_from = ic, values_from = dist) %>% 
  mutate(agree = ifelse(deltaAIC == deltaBIC, paste(deltaBIC), paste(deltaBIC, "(BIC),", deltaAIC, "(AIC)"))) %>% 
  dplyr::select(., data, agree) %>% 
  rename(Individual = data,
            `Best model` = agree)
# 
# 
# nice_prms %>% 
#   filter(., data != "pop") %>% 
#   dplyr::select(dist, data, deltaAIC,
#                 #deltaAICc,
#                 deltaBIC) %>% 
#   distinct() %>% 
#   pivot_longer(., cols = starts_with("delta"), names_to = "ic", values_to = "value") %>% 
#   filter(value > 0 & value <= 2) %>%  
#   pivot_wider(., names_from = ic, values_from = dist) %>% 
#   mutate(agree = ifelse(deltaAIC == deltaBIC, paste(deltaBIC), paste(deltaBIC, "(BIC),", deltaAIC, "(AIC)"))) %>% 
#   dplyr::select(., data, agree) %>% 
#   rename(Individual = data,
#             `Best model` = agree)

```

However, it is not as straight-forward since there are multiple competing models for certain individuals, which we consider as those within less than 2 units of the delta AIC or BIC. When using AIC, there are competing models for almost every individual, whereas with BIC, only 4 of the individuals have competing models. 
```{r fig.width=12, fig.height=4}
# To visualize the differences in the information criteria at the individual level

nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(dist, data, deltaAIC, 
                deltaAICc,
                deltaBIC) %>% 
  distinct() %>% 
  pivot_longer(., cols = starts_with("delta"), names_to = "ic", values_to = "value") %>% 
  filter(value >= 0 & value <= 2) %>%  
  mutate(plot_alpha = ifelse(value == "0", 1, 0.8)) %>% 
  ggplot(., aes(x = data, y = value, color = dist 
                #alpha = plot_alpha
                )) +
  facet_wrap(~ic) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Spectral") + 
  #scale_alpha_continuous(range = c(0.5, 1)) +
  labs(x = "") +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(title = "Distribution"))
  
```
Now we can compare what we call *models*, in which we compare a model that does complete pooling and considers all the data together, and the other model that considers individual variation, and so it fits a separate distribution to the data from each individual.

The individual variation models will fit a distribution to each individual. At one level we will use the same distribution for all individuals and only consider variation in parameters. At the next level, we will consider a change in parameters and distributions, where individuals can have different distributions, and this is what we call the mixed distribution model for individual variation. 


```{r}

# Create the dataframe that compares the models by information criteria
nice_prms %>% 
  filter(., str_detect(data, "B")) %>%
  dplyr::select(., loglik, n, dist, kpars) %>% 
  distinct() %>%
  group_by(dist) %>% 
  summarise(BIC = log(sum(n))*sum(kpars) - 2 * sum(loglik), 
            AIC = 2*(sum(kpars)) - 2*(sum(loglik)),
            AICc = 2*(sum(kpars)) - 2*(sum(loglik)) + (2* sum(kpars) * (sum(kpars)+1)/(sum(n)-sum(kpars)-1))) %>%
    mutate(deltaAIC = signif(AIC - min(AIC), 3),
         deltaAICc = signif(AICc - min(AICc), 3),
         deltaBIC = signif(BIC - min(BIC), 3),
         data = "Individual") %>% 
  full_join(., nice_prms %>%
              filter(., str_detect(data, "B", negate = TRUE)) %>% 
              dplyr::select(dist, BIC, AIC, AICc, deltaAIC, deltaAICc, deltaBIC) %>% 
              distinct() %>% 
              mutate(data = "Population")) -> popind.IC.comp


# Get the multi-distribution model fit
nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(., dist, data, loglik, n, kpars, deltaBIC) %>% 
  dplyr::filter(., deltaBIC == 0) %>% 
  distinct() %>% 
  ungroup() %>% 
  summarise(BIC = log(sum(n))*sum(kpars) - 2 * sum(loglik)) %>% 
  as.numeric() -> multi.BIC

nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(., dist, data, loglik, n, kpars, deltaAIC) %>% 
  dplyr::filter(., deltaAIC == 0) %>% 
  distinct() %>%
  ungroup() %>% 
  summarise(AIC = 2*(sum(kpars)) - 2*(sum(loglik))) %>% 
  as.numeric() -> multi.AIC

nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(., dist, data, loglik, n, kpars, deltaAICc) %>% 
  dplyr::filter(., deltaAICc == 0) %>% 
  distinct() %>%
  ungroup() %>% 
  summarise(AICc = 2*(sum(kpars)) - 2*(sum(loglik)) + (2* sum(kpars) * (sum(kpars)+1)/(sum(n)-sum(kpars)-1))) %>% 
  as.numeric()-> multi.AICc

popind.IC.comp %>% 
  dplyr::select(., dist, BIC, AIC, AICc, data) %>% 
  bind_rows(., data.frame(dist = "Multidist", BIC = multi.BIC, AIC = multi.AIC, AICc = multi.AICc, data = "Multidist")) -> model_comp_IC

```

Considering just focusing on BIC because of JMP's paper, and also in this specific case, it provides less competing models for each individual
```{r}
# Table for BIC and AIC only

model_comp_IC %>% 
  dplyr::select(data, dist, BIC, AIC) %>% 
  group_by(data) %>% 
  mutate(delta_BIC_within = signif(BIC - min(BIC), digits = 2),
         delta_AIC_within = signif(AIC - min(AIC), digits = 2)) %>% 
  ungroup() %>% 
  mutate(delta_BIC_across = signif(BIC - min(BIC), digits = 2),
         delta_AIC_across = signif(AIC - min(AIC), digits = 2)) %>% 
  dplyr::select(., -data) %>% 
  dplyr::select(dist, BIC, delta_BIC_within, delta_BIC_across, AIC, delta_AIC_within, delta_AIC_across)-> data_gt

data_gt$delta_BIC_within[which(data_gt$dist == "Multidist")] <- NA
data_gt$delta_AIC_within[which(data_gt$dist == "Multidist")] <- NA



gt_tbl <- gt(data_gt, rowname_col = "dist")

gt_tbl %>% 
  tab_header(
    title = md("**Model comparisons across levels**")
  ) %>% 
  tab_stubhead(label = "Model") %>% 
  tab_row_group(
    group = "Multidist",
    rows = 9
  ) %>% 
  tab_row_group(
    group = "Individual",
    rows = 1:4
  ) %>% 
  tab_row_group(
    group = "Population", 
    rows = 5:8
  ) %>% 
  tab_spanner(
    label = md("**BIC**"),
    columns = vars(BIC, delta_BIC_within, delta_BIC_across)
  ) %>% 
  tab_spanner(
    label = md("**AIC**"),
    columns = vars(AIC, delta_AIC_within, delta_AIC_across)
  ) %>% 
  cols_label(
    delta_BIC_within = html("&Delta; BIC<sub>within</sub>"),
    delta_BIC_across = html("&Delta; BIC<sub>across</sub>"),
    delta_AIC_within = html("&Delta; AIC<sub>within</sub>"),
    delta_AIC_across = html("&Delta; AIC<sub>across</sub>")
  )
  
  
```

## Probability distribution parameters
We can compare the different parameter values estimated for each distribution and their standard deviations. These parameters can give us some insight into some of the characteristics of each set of distances moved per minute, such as which individuals have higher means or longer tails in the distribution of their distances moved per minute. The x axis in the following plots highlights which individuals have the lowest BIC (yellow) or AIC(magenta) with that model. The blue lines show the value of that parameter for the population level model, and dashed blue lines show the upper and lower limits of that parameter estimate. Overall, at the population level, the lognormal model had the lowest AIC and BIC values.

```{r}

plot_params <- function(prm_data, best_col){
  
  prm_data%>% 
  filter(., data == "POP") %>% 
  dplyr::select(., estimate, sd) -> pop_est
  
  title <- paste(prm_data$dist, prm_data$param)
  
  if(best_col == "BIC"){
    prm_data %>% 
    filter(data != "POP") %>% 
    ggplot(., aes(x = data, y = estimate)) +
    #facet_grid(dist~param, switch = "y", scales = "free") +
    # geom_point() +
    # geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd))+
    geom_pointrange(aes(ymin = estimate - sd, ymax = estimate + sd)) +
    geom_hline(aes(yintercept = pop_est$estimate), color = "blue") +
    geom_hline(aes(yintercept = pop_est$estimate + pop_est$sd), color = "blue", linetype = "dashed") +
    geom_hline(aes(yintercept = pop_est$estimate - pop_est$sd), color = "blue", linetype = "dashed") +
    labs(x = "", y = "", subtitle = title) +
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 45, color = prm_data$best_bic)) -> plot
  }
  
  if(best_col == "AIC"){
    prm_data %>% 
    filter(data != "POP") %>% 
    ggplot(., aes(x = data, y = estimate)) +
    #facet_grid(dist~param, switch = "y", scales = "free") +
    # geom_point() +
    # geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd))+
    geom_pointrange(aes(ymin = estimate - sd, ymax = estimate + sd)) +
    geom_hline(aes(yintercept = pop_est$estimate), color = "blue") +
    geom_hline(aes(yintercept = pop_est$estimate + pop_est$sd), color = "blue", linetype = "dashed") +
    geom_hline(aes(yintercept = pop_est$estimate - pop_est$sd), color = "blue", linetype = "dashed") +
    labs(x = "", y = "", subtitle = title) +
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 45, color = prm_data$best_aic)) -> plot
  }
  return(plot)
  
          
}

```

```{r fig.height=10, fig.width=12}

nice_prms %>% 
  filter(., dist == "Exponential") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p1

nice_prms %>% 
  filter(., dist == "Gamma" & param == "SHAPE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p2

nice_prms %>% 
  filter(., dist == "Gamma" & param == "RATE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p3

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SHAPE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p4

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SCALE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p5

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "MEANLOG") %>% 
  plot_params(., best_col = "BIC") -> p6

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "SDLOG") %>% 
  plot_params(., best_col = "BIC") ->p7


plot_grid(p1, NULL, p2, p3, p4, p5, p6, p7, ncol = 2, align = "v") -> bic_grid

nice_prms %>% 
  filter(., dist == "Exponential") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p1

nice_prms %>% 
  filter(., dist == "Gamma" & param == "SHAPE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p2

nice_prms %>% 
  filter(., dist == "Gamma" & param == "RATE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p3

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SHAPE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p4

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SCALE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p5

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "MEANLOG") %>% 
  plot_params(., best_col = "AIC") -> p6

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "SDLOG") %>% 
  plot_params(., best_col = "AIC") ->p7


plot_grid(p1, NULL, p2, p3, p4, p5, p6, p7, ncol = 2, align = "v") -> aic_grid

plot_grid(bic_grid, aic_grid, ncol = 2)

```
# References






