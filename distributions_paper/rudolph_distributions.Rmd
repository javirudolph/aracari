---
title: "Variation in distance between consecutive animal locations"
abstract: |
  This should be the abstract
output:
  html_document:
    df_print: paged
  word_document:
    reference_docx: docx_ref.docx
bibliography: aracari_refs.bib
csl: ecology-letters.csl
---

# Introduction
   Movement is a fundamental ecological process, critical to an animal's biology and it's interactions with the environment. The study of animal movement tends to focus on four basic mechanistic questions: the why, when, where, and how an animal moves, and these questions can be answered at the scale of individuals or populations. A general framework for movement ecology is needed to link movement patterns and processes. Such a framework would start with a focus on movement itself and the individual's internal state, navigation capacity, motion capacity, and responses to external factors [@nathan2008movement]. The overall goal of studying movement is to find the linkages between the causes and mechanisms of animal movement, that explain spatiotemporal patterns that then feed back into various ecological and evolutionary processes. Some of these approaches involve separating different movement patterns into behavioral states, with each state having an associated set of parameters that determine that specific movement pattern[@langrock2012flexible]. Other approaches have focused on segmenting movement patterns, again in an attempt to associate specific types of movement to underlying behaviors or fine-scale environmental influences (Getz citations, Morales, more focus on the random walk models, Patterson2008). *Indeed, animal movement has consequences.  Such consequences might influence species interactions, ecological function, and vital demographic processes (survival, reproduction, dispersal) that can result in population impacts and evolutionary change (?).*
 
  Although there is variation in animal movement at the individual scale, the general focus has been on understanding the typical movement patterns, built around average measures at population levels, with the simplest approach assuming model parameters are the same across individuals, known as 'complete pooling' [@langrock2012flexible]. This focus on generality has driven to *a lack of frameworks* for understanding variation, how it is maintained, and the consequences of this variation across populations, communities, and ecosystems. Some alternatives have been considered to incorporate individual variation with 'no pooling' or 'partial pooling' approaches, where each or some individuals have their own set of parameters [@patterson2009classifying, @jonsen2006robust], however a high number of parameters quickly limits applications to high levels of individual variation. Recent calls for incorporating individual variation in movement [@snell2019consequences, @shaw2020causes] have pushed for the need to develop theory that explores different types of variation in movement patterns to understand its consequences across ecological organizational levels. In particular, when considering species interactions, incorporating individual variation becomes essential to understand how positive or negative feedback loops play into the maintenance of variation in movement as individuals respond to external factors in their environment [@shaw2020causes]. 
  

  
  
**Could I find anything pre HMMs of SSMs? Wondering what people could do before? And wondering this because we have broken up tracking periods, not continuous ones**  
Seems like I need to focus more on random walks
  
Assume weibull distributions for step length and wrapped cauchy distributions for the turning angles [@langrock2012flexible], but they tried Gamma and von Mises, respectively, but got outperformed by AIC. Used negative binomial for the state dwell times. 
  
Random walks used broadly in biology to characterize animal and cell movement. Describes que commonly used distributions for angles: von Mises, wrapped normal and wrapped Cauchy distributions. [@codling2008random]
- IDEA: so maybe use these distributions for the angles. We might see that there is not much difference in angle movement across individuals? They all tend to move forward perhaps, or something like that, and where the real differences are is in the distances. 
- Also might be intriguing to incorporate the time as a set of multiple random walks? or repeats the velocity value for that number of minutes. Maybe just using a correlated random walk, which takes into account short-term correlations in the direction of movement?
  - Codling describes this as a velocity jump process, since the markov process is the walker's velocity rather than the location. Details at the end of page 7. 
  - We consider the individuals moving in space at a constant speed $\nu$, and at each time step $\tau$ each individual changes direction and moves a distance $\delta$ in a new direction (with probability $r=\lambda\tau$), or moves a distance $\delta$ in the previous direction (with probability $q=1-\lambda\tau$). Hence, turning events occur as a Poisson process with rate $\lambda$. **This is textual from paper, so modify** Their focus is also on multiple individuals at the same time, and the density of individuals at a given time and location.
    - In my case, the distance $\delta$ would also be randomly sampled, or this are the velocities I suppose. So, perhaps it is constant speed, and the distance is actually times to account for 15, 30, etc minutes based on the data. 
    - Remember too that this velocity $\nu$ is actually the distance over time $\delta/\tau$
  - Pg11. A correlated random walk, CRW, consisting of a series of independet draws from a step length PDF and a turning angle PDF for each step, a first order markov process.
    - using mean square displacement. The effect of step length variability on MSD can be quite significant.
    - MSD being of interest to ecologists due to its relation with the diffusion coefficient D. equations 2.13 and 2.14
** Should write a conclusion about this paper**
  
# Kareiva Shigesada paper [@kareiva1983analyzing]

We focus here on developing a process to analyze animal movement sequences based on movement lengths, or step lengths, and turning angle probability distributions. Following the methods of Kareiva and Shigesada [-@kareiva1983analyzing], we assume the movement is a correlated random walk and derive expected square displacements associated to a number of consecutive moves under various probability distribution models. Unlike other studies which focus on deciphering the underlying behaviors from animal movement data, our focus was to uncover potential differences between individuals and how these differences can be obscured when modeling population-level animal movement.

"We know these simple correlated random walks, or simple movement models might not be enough to describe net displacements under more complex processes, for which developments in higher order markov processes show to be promising. However, our goal in this study was to use a simpler model and focus on potential differences when analyzing complete pooling versus no pooling data. In the context of animal-mediated seed dispersal, understanding these indivdiual level differences can help us describe and characterize the implications that different animals may have over spatial distributions of plant populations, and thus identify potential dispersers with higher or more importance, particularly when associated to long-distance seed dispersal events."

A simple correlated random walk, which involves a directional correlation between successive step movement (coddling2008)

As in other papers. Note that we adopt the convention of using upper case letters for random variables and lower case letters for possible numerical values of these random variables. 
  
# Methods

We focused our study of individual variation on the many-banded araçari (*Pterglossus pluricinctus*), a small toucan in Yasuní National Park, Ecuador. We used previously collected data from studies using radiotelemetry to estimate home ranges and evaluate the potential seed dispersal distances for the *Virola flexuosa* tree [@holbrook2011home, holbrook2007usin]. Over a period of four years, from 2001 to 2005, *P. pluricinctus* individuals were captured and radio-transmitters were attached at the base of the central tail feathers [@holbrook2011home]. Toucan locations were estimated by triangulation using receivers and hand-held antennas, with tracking periods lasting between 4-5 hours per day, locating individual birds every 15 minutes. Further details on field methods can be found in Holbrook [-@holbrook_home_2011]. It is important to note that these methods did not tack all birds simultaneously, and that although individual toucan locations were attempted every 15 minutes, this was not always possible, thus some successive locations have time categories associated to multiples of 15. For our analyses, we focused on toucans with a minimum of **20?** recorded locations, and we calculated turning angles (in radians), and the associated Euclidean step lengths in meters for each successive set of locations and turning directions. We worked under a velocity-based framework, and thus divided step lengths by the time interval during which they were recorded. This gave us a total of **600ish?** data points for turning angles and velocities.


"Location accuracy using radio telemetry may be reduced in tropical forests... conservative distance categories of 100-m increments to better represent precision. Time categories: 15, 30, 60, 90 minutes. Then calculated probability of movements made within each distance category within each time category, summed across each time category to give a final probability for each distance category."

"Distances travelled per movement bout ranged from 0 to >2000m. Strong leptokurtic distributions of movements with most being <300m. Longest recorded movement for Pluricinctus is 3665m (recorded in 30 minutes)" *this is a corresponding velocity of `r 3665/30` meters per minute.


Evaluating models with their individual-specific counterparts, using AIC or BIC. In individual-specific models the AIC results from the joint likelihood of the individual-specific models.     

Consider incorporating NAs for missed observations so we keep the regular 15 minute time interval.

```{r setup, include=FALSE}
# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(message = FALSE, warning = FALSE, 
                      eval=FALSE,
                      echo = FALSE)

```

```{r}
library(tidyverse)
library(aracari)
library(stringr)
library(fitdistrplus)
library(cowplot)
library(RColorBrewer)
library(gt)

theme_set(theme_bw())
```

```{r}
aracari_df <- readRDS("aracari_df.RDS")
```

```{r eval=FALSE}
# Which individuals will we conside? Need at least a certain number of data points to fit distributions
# Don't remove any yet, just make sure you make it clear how many data points you have for each

aracari_df %>% 
  dplyr::filter(dist != 0) %>% 
  group_by(id) %>% 
  tally()

```

```{r}
# Create my focus variable and only for individuals with 30+ observations

# focus <- "dist"
# focus <- "R2n"
 focus <- "mpm"
# focus <- "m15"

if(focus == "dist"){
  df <- aracari_df %>%
  dplyr::filter(dist != 0) %>%
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  dplyr::filter(dist != 0) %>% 
    mutate(Bird_ID = paste0("B", id),
           focus = dist)
  
  dist.used <- c("lnorm", "weibull", "cauchy")
  param <- c("meanlog", "sdlog", "shape", "scale", "location", "scale")
  dist <- c("lnorm", "lnorm", "weibull", "weibull", "cauchy", "cauchy")
  kpars <- c(2, 2, 2, 2, 2, 2)
}

if(focus == "R2n"){
  df <- aracari_df %>%
  dplyr::filter(R2n != 0) %>%
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  dplyr::filter(R2n != 0) %>% 
    mutate(Bird_ID = paste0("B", id),
           focus = sqrt(R2n))
  
  dist.used <- c( "exp", "gamma", "weibull", "lnorm")
  param <- c("rate", "shape", "rate", "shape", "scale", "meanlog", "sdlog")
  dist <- c("exp", "gamma", "gamma", "weibull", "weibull", "lnorm", "lnorm")
  kpars <- c(1, 2, 2, 2, 2, 2, 2)
}

if(focus == "mpm"){
  df <- aracari_df %>% 
  dplyr::filter(dist != 0) %>% 
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  mutate(tmin = dt/60,
         mpm = dist/tmin,
         Bird_ID = paste0("B", id),
         focus = mpm) %>% 
  dplyr::filter(., mpm != 0)
  
  dist.used <- c( "exp", "gamma", "weibull", "lnorm")
  param <- c("rate", "shape", "rate", "shape", "scale", "meanlog", "sdlog")
  dist <- c("exp", "gamma", "gamma", "weibull", "weibull", "lnorm", "lnorm")
  kpars <- c(1, 2, 2, 2, 2, 2, 2)
}

if(focus == "m15"){
  df <- aracari_df %>% 
  dplyr::filter(dist != 0) %>% 
  group_by(id) %>%
  filter(., n() >= 30) %>%
  ungroup() %>%
  mutate(tmin = dt/60,
         mpm = dist/tmin,
         m15 = mpm*15,
         Bird_ID = paste0("B", id),
         focus = m15) %>% 
  dplyr::filter(., m15 != 0)
  
  dist.used <- c( "exp", "gamma", "weibull", "lnorm")
  param <- c("rate", "shape", "rate", "shape", "scale", "meanlog", "sdlog")
  dist <- c("exp", "gamma", "gamma", "weibull", "weibull", "lnorm", "lnorm")
  kpars <- c(1, 2, 2, 2, 2, 2, 2)
}


```

\newpage
# Questions  
* What is the distribution of distances between consecutive recorded locations? 
* Are there differences between individuals or can we use the same distribution to describe these distances between two locations?

The reasoning behind this, is that distances between locations can be used later on to describe variation in step length when simulating animal movement under simple models such as random walk.

### MPM: meters per minute
Due to the variation in time between consecutive locations, we scaled the step lengths to displacement in one minute.
**Visualize the variation and distribution of these distances between locations**  

```{r}
 df %>%
  ggplot(., aes((x = focus))) +
  geom_line(aes(x = focus, group = id), stat = "density", alpha = 0.4) +
  geom_line(stat = "density", color = "#1191d1", lwd = 1) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  labs(y = "Density", x = paste(focus)) +
  theme_classic()


```

```{r}
df %>%
  ggplot(., aes(x = focus)) +
  facet_wrap(~id) +
  geom_line(stat = "density") +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  labs(y = "Density", x = paste(focus)) +
  theme_classic()
```


# Fit distribution  
Useful resource to use the package [fitdistrplus](https://cran.r-project.org/web/packages/fitdistrplus/vignettes/FAQ.html)
Also check [this](https://stackoverflow.com/questions/37152482/power-law-fitted-by-fitdistr-function-in-package-fitdistrplus). Should I do the opimization by hand?

```{r}

# Fit the four models for the population level data
  
pop <- lapply(dist.used, function(x){fitdist(df$focus, distr = x)})

# Get data for qqplot at population level
qqpop <- qqcomp(pop, plotstyle = "ggplot")$data
names(qqpop) <- c("theoretical", "fdist", "empirical")

# Save the population level parameters

build_fits_df <- function(x){

  nm <- deparse(substitute(x))

  x %>%
    map(., `[`, c("estimate", "sd", "loglik", "n")) %>%
    map(as_tibble) %>%
    bind_rows()
}

prms.df <- build_fits_df(pop) %>% 
  mutate(param = param,
         dist = dist,
         kpars = kpars,
         data = "pop")

# Repeating the process for data at the individual level
inds <- unique(as.character(df$Bird_ID))

# Fitting the four distributions for each individual and saving the parameters in a dataframe
ind.models <- NULL
for(i in 1:length(inds)){
  fit.df <- df %>% 
    dplyr::filter(., Bird_ID == inds[i])
  
  fit <- lapply(dist.used, function(x){fitdist(fit.df$focus, distr = x)})
  
  ind.models[[i]] <- fit
  
  assign(paste(inds[i]), fit)
  
  fit.prms <- build_fits_df(fit) %>% 
  mutate(param = param,
         dist = dist,
         kpars = kpars,
         data = inds[i])
  
  prms.df <- bind_rows(prms.df, fit.prms)
}


# Getting qqplot data for each individual
inds.models.data <- NULL
for(i in 1:length(inds)){
  x <- ind.models[[i]]
  
  qqdata <- qqcomp(x, plotstyle = "ggplot")$data %>% 
    mutate(data = inds[i])
  names(qqdata) <- c("theoretical", "fdist", "empirical", "individual")
  inds.models.data <- bind_rows(inds.models.data, qqdata)
}
```


```{r ind_QQplots, fig.width=12, fig.height=5}

qqpop %>% 
  mutate(individual = "POPULATION") %>% 
  ggplot(., aes(x = theoretical, y = empirical, color = fdist)) +
  facet_wrap(~individual) +
  #coord_equal() +
  geom_point(shape = 16, alpha = 0.8, size = 4) +
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "Model Quantiles",
         y = "Empirical Quantiles") +
  #scale_color_viridis_d() +
  scale_color_brewer(palette = "Spectral", name = "Distribution") +
  theme(legend.position = c(0.8, 0.2)) +
  NULL -> a

inds.models.data%>% 
  ggplot(., aes(x = theoretical, y = empirical, color = fdist)) +
  #facet_grid(individual~fdist) +
  facet_wrap(~individual)+
    geom_point(shape = 16, alpha = 0.8, size = 3) +
    geom_abline(intercept = 0, slope = 1) +
    labs(x = "Model Quantiles",
         y = "Empirical Quantiles") +
  #scale_color_viridis_d() +
  scale_color_brewer(palette = "Spectral") +
  theme(legend.position = "none") +
  guides(color = guide_legend(title = "Distribution")) +
  #coord_equal() +
  NULL -> b

plot_grid(a,b, rel_widths = c(1,2), labels = "AUTO")

```

## Information criteria

```{r}

ic_calc <- function(n, kpars, loglik){
  
  BIC <- log(n)*kpars - 2*loglik
  AIC <- 2*kpars - 2*loglik
  AICc <- AIC + (2* kpars * (kpars+1)/(n-kpars-1))
}


prms.df %>% 
  mutate(param = str_to_upper(param), 
         dist = str_replace(dist, "exp", "Exponential"),
         dist = str_replace(dist, "gamma", "Gamma"),
         dist = str_replace(dist, "weibull", "Weibull"),
         dist = str_replace(dist, "lnorm", "Lognormal"),
         data = str_replace(data, "pop", "POP")) %>% 
  mutate(BIC = log(n)*kpars - 2*loglik,
         AIC = 2*kpars - 2*loglik,
         AICc = AIC + (2* kpars * (kpars+1)/(n-kpars-1))) %>% 
  group_by(data) %>% 
  mutate(deltaBIC = signif(BIC - min(BIC), 3),
         deltaAIC = signif(AIC - min(AIC), 3),
         deltaAICc = signif(AICc - min(AICc), 3)) %>% 
  mutate(best_bic = ifelse(deltaBIC == 0, "#ffaa00", "black"),
         best_aic = ifelse(deltaAIC == 0, "#fc0362", "black")) -> nice_prms
```

So, for each individual, which is the best fitting distribution, based on AIC or BIC? This is the distribution with the lowest AIC or BIC score for each individual. These are basically the distributions that will be used for the mixed distribution model later on. 

```{r}
nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(dist, data, deltaAIC,
                #deltaAICc,
                deltaBIC) %>% 
  distinct() %>% 
  pivot_longer(., cols = starts_with("delta"), names_to = "ic", values_to = "value") %>% 
  filter(value == 0) %>%  
  pivot_wider(., names_from = ic, values_from = dist) %>% 
  mutate(agree = ifelse(deltaAIC == deltaBIC, paste(deltaBIC), paste(deltaBIC, "(BIC),", deltaAIC, "(AIC)"))) %>% 
  dplyr::select(., data, agree) %>% 
  rename(Individual = data,
            `Best model` = agree)
# 
# 
# nice_prms %>% 
#   filter(., data != "pop") %>% 
#   dplyr::select(dist, data, deltaAIC,
#                 #deltaAICc,
#                 deltaBIC) %>% 
#   distinct() %>% 
#   pivot_longer(., cols = starts_with("delta"), names_to = "ic", values_to = "value") %>% 
#   filter(value > 0 & value <= 2) %>%  
#   pivot_wider(., names_from = ic, values_from = dist) %>% 
#   mutate(agree = ifelse(deltaAIC == deltaBIC, paste(deltaBIC), paste(deltaBIC, "(BIC),", deltaAIC, "(AIC)"))) %>% 
#   dplyr::select(., data, agree) %>% 
#   rename(Individual = data,
#             `Best model` = agree)

```

However, it is not as straight-forward since there are multiple competing models for certain individuals, which we consider as those within less than 2 units of the delta AIC or BIC. When using AIC, there are competing models for almost every individual, whereas with BIC, only 4 of the individuals have competing models. 
```{r fig.width=12, fig.height=4}
# To visualize the differences in the information criteria at the individual level

nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(dist, data, deltaAIC, 
                deltaAICc,
                deltaBIC) %>% 
  distinct() %>% 
  pivot_longer(., cols = starts_with("delta"), names_to = "ic", values_to = "value") %>% 
  filter(value >= 0 & value <= 2) %>%  
  mutate(plot_alpha = ifelse(value == "0", 1, 0.8)) %>% 
  ggplot(., aes(x = data, y = value, color = dist 
                #alpha = plot_alpha
                )) +
  facet_wrap(~ic) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Spectral") + 
  #scale_alpha_continuous(range = c(0.5, 1)) +
  labs(x = "") +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(title = "Distribution"))
  
```
Now we can compare what we call *models*, in which we compare a model that does complete pooling and considers all the data together, and the other model that considers individual variation, and so it fits a separate distribution to the data from each individual.

The individual variation models will fit a distribution to each individual. At one level we will use the same distribution for all individuals and only consider variation in parameters. At the next level, we will consider a change in parameters and distributions, where individuals can have different distributions, and this is what we call the mixed distribution model for individual variation. 


```{r}

# Create the dataframe that compares the models by information criteria
nice_prms %>% 
  filter(., str_detect(data, "B")) %>%
  dplyr::select(., loglik, n, dist, kpars) %>% 
  distinct() %>%
  group_by(dist) %>% 
  summarise(BIC = log(sum(n))*sum(kpars) - 2 * sum(loglik), 
            AIC = 2*(sum(kpars)) - 2*(sum(loglik)),
            AICc = 2*(sum(kpars)) - 2*(sum(loglik)) + (2* sum(kpars) * (sum(kpars)+1)/(sum(n)-sum(kpars)-1))) %>%
    mutate(deltaAIC = signif(AIC - min(AIC), 3),
         deltaAICc = signif(AICc - min(AICc), 3),
         deltaBIC = signif(BIC - min(BIC), 3),
         data = "Individual") %>% 
  full_join(., nice_prms %>%
              filter(., str_detect(data, "B", negate = TRUE)) %>% 
              dplyr::select(dist, BIC, AIC, AICc, deltaAIC, deltaAICc, deltaBIC) %>% 
              distinct() %>% 
              mutate(data = "Population")) -> popind.IC.comp


# Get the multi-distribution model fit
nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(., dist, data, loglik, n, kpars, deltaBIC) %>% 
  dplyr::filter(., deltaBIC == 0) %>% 
  distinct() %>% 
  ungroup() %>% 
  summarise(BIC = log(sum(n))*sum(kpars) - 2 * sum(loglik)) %>% 
  as.numeric() -> multi.BIC

nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(., dist, data, loglik, n, kpars, deltaAIC) %>% 
  dplyr::filter(., deltaAIC == 0) %>% 
  distinct() %>%
  ungroup() %>% 
  summarise(AIC = 2*(sum(kpars)) - 2*(sum(loglik))) %>% 
  as.numeric() -> multi.AIC

nice_prms %>% 
  filter(., str_detect(data, "B")) %>% 
  dplyr::select(., dist, data, loglik, n, kpars, deltaAICc) %>% 
  dplyr::filter(., deltaAICc == 0) %>% 
  distinct() %>%
  ungroup() %>% 
  summarise(AICc = 2*(sum(kpars)) - 2*(sum(loglik)) + (2* sum(kpars) * (sum(kpars)+1)/(sum(n)-sum(kpars)-1))) %>% 
  as.numeric()-> multi.AICc

popind.IC.comp %>% 
  dplyr::select(., dist, BIC, AIC, AICc, data) %>% 
  bind_rows(., data.frame(dist = "Multidist", BIC = multi.BIC, AIC = multi.AIC, AICc = multi.AICc, data = "Multidist")) -> model_comp_IC

```

Considering just focusing on BIC because of JMP's paper, and also in this specific case, it provides less competing models for each individual
```{r}
# Table for BIC and AIC only

model_comp_IC %>% 
  dplyr::select(data, dist, BIC, AIC) %>% 
  group_by(data) %>% 
  mutate(delta_BIC_within = signif(BIC - min(BIC), digits = 2),
         delta_AIC_within = signif(AIC - min(AIC), digits = 2)) %>% 
  ungroup() %>% 
  mutate(delta_BIC_across = signif(BIC - min(BIC), digits = 2),
         delta_AIC_across = signif(AIC - min(AIC), digits = 2)) %>% 
  dplyr::select(., -data) %>% 
  dplyr::select(dist, BIC, delta_BIC_within, delta_BIC_across, AIC, delta_AIC_within, delta_AIC_across)-> data_gt

data_gt$delta_BIC_within[which(data_gt$dist == "Multidist")] <- NA
data_gt$delta_AIC_within[which(data_gt$dist == "Multidist")] <- NA



gt_tbl <- gt(data_gt, rowname_col = "dist")

gt_tbl %>% 
  tab_header(
    title = md("**Model comparisons across levels**")
  ) %>% 
  tab_stubhead(label = "Model") %>% 
  tab_row_group(
    group = "Multidist",
    rows = 9
  ) %>% 
  tab_row_group(
    group = "Individual",
    rows = 1:4
  ) %>% 
  tab_row_group(
    group = "Population", 
    rows = 5:8
  ) %>% 
  tab_spanner(
    label = md("**BIC**"),
    columns = vars(BIC, delta_BIC_within, delta_BIC_across)
  ) %>% 
  tab_spanner(
    label = md("**AIC**"),
    columns = vars(AIC, delta_AIC_within, delta_AIC_across)
  ) %>% 
  cols_label(
    delta_BIC_within = html("&Delta; BIC<sub>within</sub>"),
    delta_BIC_across = html("&Delta; BIC<sub>across</sub>"),
    delta_AIC_within = html("&Delta; AIC<sub>within</sub>"),
    delta_AIC_across = html("&Delta; AIC<sub>across</sub>")
  )
  
  
```

## Probability distribution parameters
We can compare the different parameter values estimated for each distribution and their standard deviations. These parameters can give us some insight into some of the characteristics of each set of distances moved per minute, such as which individuals have higher means or longer tails in the distribution of their distances moved per minute. The x axis in the following plots highlights which individuals have the lowest BIC (yellow) or AIC(magenta) with that model. The blue lines show the value of that parameter for the population level model, and dashed blue lines show the upper and lower limits of that parameter estimate. Overall, at the population level, the lognormal model had the lowest AIC and BIC values.

```{r}

plot_params <- function(prm_data, best_col){
  
  prm_data%>% 
  filter(., data == "POP") %>% 
  dplyr::select(., estimate, sd) -> pop_est
  
  title <- paste(prm_data$dist, prm_data$param)
  
  if(best_col == "BIC"){
    prm_data %>% 
    filter(data != "POP") %>% 
    ggplot(., aes(x = data, y = estimate)) +
    #facet_grid(dist~param, switch = "y", scales = "free") +
    # geom_point() +
    # geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd))+
    geom_pointrange(aes(ymin = estimate - sd, ymax = estimate + sd)) +
    geom_hline(aes(yintercept = pop_est$estimate), color = "blue") +
    geom_hline(aes(yintercept = pop_est$estimate + pop_est$sd), color = "blue", linetype = "dashed") +
    geom_hline(aes(yintercept = pop_est$estimate - pop_est$sd), color = "blue", linetype = "dashed") +
    labs(x = "", y = "", subtitle = title) +
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 45, color = prm_data$best_bic)) -> plot
  }
  
  if(best_col == "AIC"){
    prm_data %>% 
    filter(data != "POP") %>% 
    ggplot(., aes(x = data, y = estimate)) +
    #facet_grid(dist~param, switch = "y", scales = "free") +
    # geom_point() +
    # geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd))+
    geom_pointrange(aes(ymin = estimate - sd, ymax = estimate + sd)) +
    geom_hline(aes(yintercept = pop_est$estimate), color = "blue") +
    geom_hline(aes(yintercept = pop_est$estimate + pop_est$sd), color = "blue", linetype = "dashed") +
    geom_hline(aes(yintercept = pop_est$estimate - pop_est$sd), color = "blue", linetype = "dashed") +
    labs(x = "", y = "", subtitle = title) +
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 45, color = prm_data$best_aic)) -> plot
  }
  return(plot)
  
          
}

```

```{r fig.height=10, fig.width=12}

nice_prms %>% 
  filter(., dist == "Exponential") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p1

nice_prms %>% 
  filter(., dist == "Gamma" & param == "SHAPE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p2

nice_prms %>% 
  filter(., dist == "Gamma" & param == "RATE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p3

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SHAPE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p4

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SCALE") %>% 
  plot_params(., best_col = "BIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p5

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "MEANLOG") %>% 
  plot_params(., best_col = "BIC") -> p6

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "SDLOG") %>% 
  plot_params(., best_col = "BIC") ->p7


plot_grid(p1, NULL, p2, p3, p4, p5, p6, p7, ncol = 2, align = "v") -> bic_grid

nice_prms %>% 
  filter(., dist == "Exponential") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p1

nice_prms %>% 
  filter(., dist == "Gamma" & param == "SHAPE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p2

nice_prms %>% 
  filter(., dist == "Gamma" & param == "RATE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL -> p3

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SHAPE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p4

nice_prms %>% 
  filter(., dist == "Weibull" & param == "SCALE") %>% 
  plot_params(., best_col = "AIC") +
  #theme(axis.text.x = element_blank()) +
  NULL-> p5

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "MEANLOG") %>% 
  plot_params(., best_col = "AIC") -> p6

nice_prms %>% 
  filter(., dist == "Lognormal" & param == "SDLOG") %>% 
  plot_params(., best_col = "AIC") ->p7


plot_grid(p1, NULL, p2, p3, p4, p5, p6, p7, ncol = 2, align = "v") -> aic_grid

plot_grid(bic_grid, aic_grid, ncol = 2)

```
# References






