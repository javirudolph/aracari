---
title: "Ch3 process"
author: "Javiera Rudolph"
date: "1/20/2022"
output:
  html_document:
  toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.width=4, fig.height=3)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r}
# Libraries -----------------------------------------------
set.seed(20220201)

library(aracari)
library(dplyr)
library(ggplot2)
library(cowplot)
library(tidyr)
library(purrr)
library(fitdistrplus)
```

# Methods  
We use a theoretical and simulation-based approach to understand how estimates of long-distance movements change with sample size and the choice of threshold value to describe a long-distance movement. We focus on testing the effectiveness of using a Lomax distribution to describe the probability of long-distance movements arising from complex mixture models incorporating individual variability. 

## Simulating group variability with a finite mixture distribution model  

Mixture distribution models are commonly used to describe the distribution of a process generated by multiple subpopulations. In the case of seed dispersal, mixture models have been used to describe the multimodality of seed dispersal kernels, specifically when accounting for disperser behaviors which lead to seed clumping. In the case of animal movement, mixture models and extensions are used to describe the multiple behaviors associated to an individual in movement. In our case, we use a mixture model to describe different subgroups which differ from each other in the distribution of step lengths. This is a theoretical example, a toy model, which includes four groups, each described by a lognormal distribution with a different set of parameters. 


```{r MixtureParamFX}
## Parameter functions for mixture ----------------------------------------------------
# Building this function so that we get the parameters for a desired mean and standard deviation.
desired_mean_sd <- function(mean, sd){

  mu <- log(mean*2/(sqrt(mean*2+sd*2)))
  sigsq <- log(1+(sd*2/mean*2))

  return(data.frame(mu = mu, sigsq = sigsq))
}

exp.val <- function(mu, sigsq){
  exp(mu + sqrt(sigsq))
}
```



<!-- Thinking of things that need to be tested: like if we change the different weights. Also, varying the parameters for the distributions. Is this essentially a sensitivity analysis? -->


```{r}
# MIXTURE MODEL --------------------------------------------------
## Mixture proportions ----------------------------------------
# We assume four categories of individuals with increasing movement.
pis <- c( 0.1, 0.2, 0.3, 0.4)

## Mixture components --------------------------------------
# Since movement lengths are only positive, we use a lognormal distribution to describe them

# TEST 1

pars <- desired_mean_sd(mean = c(160, 300, 600, 1000), sd = c(90, 120, 160, 200))

# thinking of how to calculate the cdf for a mixture
#plnorm(80, meanlog = pars$mu[1], sdlog = pars$sigsq[1], lower.tail = FALSE)

dens_cols <- c("#264653", "#2a9d8f", "#f4a261", "#e76f51")

### PLOT the mixture components -------------------------------
lnorm_densities <- purrr::map(1:4, function(y) stat_function(fun = dlnorm,
                                                             args = list(meanlog = pars$mu[y], sdlog = pars$sigsq[y]),
                                                             color = dens_cols[y], size=1))
ggplot() +
  lnorm_densities +
  theme_minimal() +
  labs(y = "Density") +
  lims(x = c(0, 150), y = c(0,0.05)) -> densities_plot
densities_plot

```

We sample step lengths and generate a single distribution using the following weights: `r pis`

```{r fig.width=8}
# SAMPLING --------------------------------------------------------------

samp.size <- 50000
all.samples <- rep(0,samp.size)
categ <- rep(0,samp.size)

for(i in 1:samp.size){

  which.cat <- sample(1:4, size=1, replace=TRUE, prob=pis)
  all.samples[i] <- rlnorm(n=1,meanlog=pars$mu[which.cat], sdlog=pars$sigsq[which.cat])
  categ[i] <- which.cat
}


## PLOT the samples ---------------------------------------------
# This is our f(x) from the main text
data.tail <- data.frame(values = all.samples, y = 100) %>% arrange(desc(values)) %>% filter(values >=250)
#head(data.tail)

ggplot(data.frame(all.samples), aes(x = all.samples)) +
  geom_histogram(bins = 100) +
  geom_point(data = data.tail[1:50,], aes(x = values, y = y), color = "black", alpha = 0.5) +
  labs(y = "Frequency", x = "Distance") +
  #lims(x = c(0, 150)) +
  theme_minimal() -> sampleshist

ggplot(data.frame(all.samples), aes(x = all.samples)) +
  geom_density() +
  labs(y = "Density", x = "Distance") +
  lims(x = c(0, 150), y = c(0,0.05)) +
  theme_minimal() -> samplesdens

plot_grid(sampleshist, samplesdens)

```


Testing the results if using purrr instead of a for loop:  

```{r fig.width=8}
#Trying an alternative way of drawing the samples

cat.samp.sizes <- samp.size*pis

purrrsampls <- tibble(gID = c(1:4), pars) %>% 
  mutate(data = purrr::map(gID, function(y) rlnorm(cat.samp.sizes[y], mu[y], sigsq[y]))) 


purrrsampls %>% 
  dplyr::select(., gID, data) %>% 
  unnest(cols = c(data)) -> simplsamps

simplsamps %>%
  ggplot(., aes(x = data)) +
  geom_histogram(bins = 100) +
  geom_point(data = data.tail[1:50,], aes(x = values, y = y), color = "black", alpha = 0.5) +
  labs(y = "Frequency", x = "Distance") +
  #lims(x = c(0, 150)) +
  theme_minimal() -> sampleshist

simplsamps %>% 
  ggplot(., aes(x = data)) +
  geom_density() +
  labs(y = "Density", x = "Distance") +
  lims(x = c(0, 150), y = c(0,0.05)) +
  theme_minimal() -> samplesdens

plot_grid(sampleshist, samplesdens)



```

```{r LOMAX}
# All the lomax functions

# FITTING -------------------------------------
# We use a Lomax distribution, which is actually a special case of generalized Pareto distribution
# The Lomax or Patro type II has support on x=0, and is a heavy tail distribution
# Jose's document shows that it arises from incorporating variability in the rate parameter of an exponential distribution, using a gamma distribution - which he calls the hierarchical
# It only has two parameters, alpha(shape) and lambda(scale)

lomax.pdf <- function(x,alpha,k, log.scale=FALSE){
  
  if(log.scale==FALSE){out <- (k/(alpha+x))*(alpha/(alpha+x))^k
  }else{
    out <- log(k) + k*log(alpha) - (k+1)*log(alpha+x)
  }
  
  return(out)
}


lomax.cdf <- function(x,alpha,k){
  
  return(1-(alpha/(alpha+x))^k)
  
}


ft.nllike2 <- function(guess=init.betas, designmat=designmat,Y=Y){
  
  nbetasp1      <- length(init.betas)
  k             <- exp(guess[1])
  Xbeta         <- designmat%*%guess[2:nbetasp1]
  alphas        <- exp(Xbeta) # because alpha = ln(X*betas)
  n             <- length(Y)
  
  #sumlogapy     <- sum(log(alphas+Y))
  #k.hat         <- n/(sumlogapy - sum(Xbeta))
  lnft.yis     <- lomax.pdf(x=Y, alpha=alphas,k=k,log.scale=TRUE)
  lnL           <- sum(lnft.yis)
  nll           <- -lnL
  
  return(nll)
}


lomax.glm <- function(formula=~1, my.dataf, response){
  
  Y           <- response
  n           <- length(Y)
  designmat   <- model.matrix(formula, data=my.dataf)
  nbetas      <- ncol(designmat)
  init.betas  <- c(4,rep(5,nbetas))
  
  opt.out <- optim(par=init.betas, fn=ft.nllike2, method = "Nelder-Mead",
                   designmat=designmat, Y=Y)
  
  mles          <- opt.out$par
  nll.hat       <- opt.out$value
  BIC.mod       <- 2*nll.hat + nbetas*log(length(Y))
  Xbeta.hat     <- designmat%*%mles[-1]
  alphas.hat    <- exp(Xbeta.hat)
  #sumlogapy.hat <- sum(log(alphas.hat+Y))
  k.hat         <-  mles[1] #n/(sumlogapy.hat - sum(Xbeta.hat))
  
  out.list <- list(opt.out = opt.out, designmat=designmat,Y=Y, mles=mles, nll.hat=nll.hat, BIC.mod = BIC.mod,
                   alphas.hat=alphas.hat, k.hat=k.hat,data=my.dataf)
  
  return(out.list)
  
}

```

Running the experiment once:  

```{r echo=TRUE}
###################################
# Run the experiments with different sample sizes and tail thresholds

samp.sizes <- c(80, 200, 500, 800, 1000, 1600)
num.ns <- length(samp.sizes)
q.tests <- c(100, 250,500,1000)
num.qs <- length(q.tests)

all.sampsizes <- rep(samp.sizes,each=num.qs)
all.qtests <- rep(q.tests,num.ns)
ntests <- length(all.sampsizes)

cdfs.hat <- rep(0, ntests)


for(i in 1:ntests){
  ith.n   <- all.sampsizes[i]
  ith.samples <- data.frame(x = sample(all.samples, ith.n))
  
  formula=~1
  my.dataf = ith.samples
  response = ith.samples$x
  
  
  Y           <- response
  n           <- length(Y)
  designmat   <- model.matrix(formula, data=my.dataf)
  nbetas      <- ncol(designmat)
  init.betas  <- c(4,rep(5,nbetas))
  
  opt.out <- optim(par=init.betas, fn=ft.nllike2, method = "Nelder-Mead",
                   designmat=designmat, Y=Y)
  
  mles          <- opt.out$par
  nll.hat       <- opt.out$value
  BIC.mod       <- 2*nll.hat + nbetas*log(length(Y))
  Xbeta.hat     <- designmat%*%mles[-1]
  alphas.hat    <- exp(Xbeta.hat)
  #sumlogapy.hat <- sum(log(alphas.hat+Y))
  k.hat         <-  mles[1] #n/(sumlogapy.hat - sum(Xbeta.hat))
  
  mod1 <- list(opt.out = opt.out, designmat=designmat,Y=Y, mles=mles, nll.hat=nll.hat, BIC.mod = BIC.mod,
                   alphas.hat=alphas.hat, k.hat=k.hat,data=my.dataf)
  

  ith.a <- mod1$alphas.hat[1]
  ith.k <- mod1$k.hat

  ith.q <- all.qtests[i]
  ith.cdf <- 1- lomax.cdf(x = ith.q, alpha = ith.a, k = ith.k)
  cdfs.hat[i] <- ith.cdf

}


true.cdfs <- rep(0,num.qs)
for(i in 1:num.qs){

  iq <- q.tests[i]

  true.cdfs[i] <- sum(all.samples>iq)/length(all.samples)

}

all.true.cdfs <- rep(true.cdfs,num.ns)


sim.test.df <- data.frame(all.sampsizes=all.sampsizes, all.qtests=all.qtests,cdfs.hat=cdfs.hat,
                          true.cdfs = all.true.cdfs)
```





