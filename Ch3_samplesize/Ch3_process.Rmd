---
title: "Ch3 process"
author: "Javiera Rudolph"
date: "1/20/2022"
output:
  html_document:
  toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.width=4, fig.height=3)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r}
# Libraries -----------------------------------------------
set.seed(20220201)

library(aracari)
library(dplyr)
library(ggplot2)
library(cowplot)
library(tidyr)
library(purrr)
library(fitdistrplus)
```

# Methods  
We use a theoretical and simulation-based approach to understand how estimates of long-distance movements change with sample size and the choice of threshold value to describe a long-distance movement. We focus on testing the effectiveness of using a Lomax distribution to describe the probability of long-distance movements arising from complex mixture models incorporating individual variability. 

## Simulating group variability with a finite mixture distribution model  

Mixture distribution models are commonly used to describe the distribution of a process generated by multiple subpopulations. In the case of seed dispersal, mixture models have been used to describe the multimodality of seed dispersal kernels, specifically when accounting for disperser behaviors which lead to seed clumping. In the case of animal movement, mixture models and extensions are used to describe the multiple behaviors associated to an individual in movement. In our case, we use a mixture model to describe different subgroups which differ from each other in the distribution of step lengths. This is a theoretical example, a toy model, which includes four groups, each described by a lognormal distribution with a different set of parameters. 


```{r MixtureParamFX}
## Parameter functions for mixture ----------------------------------------------------
# Building this function so that we get the parameters for a desired mean and standard deviation.
desired_mean_sd <- function(mean, sd){

  mu <- log(mean*2/(sqrt(mean*2+sd*2)))
  sigsq <- log(1+(sd*2/mean*2))

  return(data.frame(mu = mu, sigsq = sigsq))
}

exp.val <- function(mu, sigsq){
  exp(mu + sqrt(sigsq))
}
```



<!-- Thinking of things that need to be tested: like if we change the different weights. Also, varying the parameters for the distributions. Is this essentially a sensitivity analysis? -->


```{r}
# MIXTURE MODEL --------------------------------------------------
## Mixture proportions ----------------------------------------
# We assume four categories of individuals with increasing movement.
pis <- c( 0.1, 0.2, 0.3, 0.4)

## Mixture components --------------------------------------
# Since movement lengths are only positive, we use a lognormal distribution to describe them

# TEST 1

pars <- desired_mean_sd(mean = c(160, 300, 600, 1000), sd = c(90, 120, 160, 250))


# thinking of how to calculate the cdf for a mixture
#plnorm(80, meanlog = pars$mu[1], sdlog = pars$sigsq[1], lower.tail = FALSE)

dens_cols <- c("#264653", "#2a9d8f", "#f4a261", "#e76f51")

### PLOT the mixture components -------------------------------
lnorm_densities <- purrr::map(1:4, function(y) stat_function(fun = dlnorm,
                                                             args = list(meanlog = pars$mu[y], sdlog = pars$sigsq[y]),
                                                             color = dens_cols[y], size=1))
ggplot() +
  lnorm_densities +
  theme_minimal() +
  labs(y = "Density") +
  lims(x = c(0, 150), y = c(0,0.05)) -> densities_plot
densities_plot


# ### PLOT the mixture components CDF -------------------------------
# lnorm_cdfs <- purrr::map(1:4, function(y) stat_function(fun = plnorm,
#                                                              args = list(meanlog = pars$mu[y], sdlog = pars$sigsq[y]),
#                                                              color = dens_cols[y], size=1))
# ggplot() +
#   lnorm_cdfs +
#   theme_minimal() +
#   labs(y = "Density") +
#   lims(x = c(50, 250), y = c(0.9, 1))
#        
```

We sample step lengths and generate a single distribution using the following weights: `r pis`

```{r fig.width=8}
# Using purrr to get samples instead of a for loop. More efficient.
samp.size <- 50000
cat.samp.sizes <- samp.size*pis

purrrsampls <- tibble(gID = c(1:4), pars) %>% 
  mutate(data = purrr::map(gID, function(y) rlnorm(cat.samp.sizes[y], mu[y], sigsq[y]))) 

purrrsampls %>% 
  dplyr::select(., gID, data) %>% 
  unnest(cols = c(data)) -> simplsamps

data.tail <- data.frame(values = simplsamps$data, y = 100) %>% arrange(desc(values)) %>% filter(values >=250)

simplsamps %>%
  ggplot(., aes(x = data)) +
  geom_histogram(bins = 100) +
  geom_point(data = data.tail[1:50,], aes(x = values, y = y), color = "black", alpha = 0.5) +
  labs(y = "Frequency", x = "Distance") +
  #lims(x = c(0, 150)) +
  theme_minimal() -> sampleshist

simplsamps %>% 
  ggplot(., aes(x = data)) +
  geom_density() +
  labs(y = "Density", x = "Distance") +
  lims(x = c(0, 150), y = c(0,0.05)) +
  theme_minimal() -> samplesdens

plot_grid(sampleshist, samplesdens)



```

Answering some questions about the underlying process. 

What is the true CDF of the function that generates the process? This is the CDF of the mixture distribution, which is straightforward to calculate.

$$
f(x) = \sum_{i=1}^n w_i p_i(x) \Rightarrow F(x) = \sum_{i=1}^n w_i P_i(x)
$$

```{r}
thresh.vals <- c(100, 250, 500, 750, 1000)
samp.sizes <- c(80, 200, 500, 800, 1000, 1600)

```


The thresholds we want to consider are the following:`r thresh.vals` and the weights haven't changed. So, we get the cdf for each $p_i(x)$ for each threshold. This means we get $P(x >= c(100, 250, 500, 1000))$ for each curve and weigh it. 

```{r echo=FALSE, eval = FALSE}
# Getting weird stuff so taking this step by step
y <- 4

ggplot() +
  stat_function(fun = dlnorm, args = list(meanlog = pars$mu[y], sdlog = pars$sigsq[y]),
                                                             color = dens_cols[y], size=1) +
  theme_minimal() +
  labs(y = "Density") +
  lims(x = c(0, 150), y = c(0,0.05))

cdfs <- NULL
for(y in 1:4){
  a <- plnorm(thresh.vals, meanlog = pars$mu[y], sdlog = pars$sigsq[y], lower.tail = FALSE)
  b <- plnorm(thresh.vals, meanlog = pars$mu[y], sdlog = pars$sigsq[y], lower.tail = FALSE) * pis[y]

  c <- rbind(a,b)
  
  out <- cbind(c, c(y, y))
  
  cdfs <- rbind.data.frame(cdfs, out)
}
names(cdfs) <- c(thresh.vals, "curve")
cdfs

```


```{r}
weighted.cdfs <- NULL
for(y in 1:4){
  a <- plnorm(thresh.vals, meanlog = pars$mu[y], sdlog = pars$sigsq[y], lower.tail = FALSE) * pis[y]
  weighted.cdfs <- rbind(weighted.cdfs, a)
}
w.cdfs <- colSums(weighted.cdfs)


tibble(thresh.vals) %>% 
  mutate(w.cdfs = w.cdfs,
         w.cdfs = round(w.cdfs, 5),
         samp.n = map_dbl(1:length(thresh.vals), function(y) length(which(simplsamps$data >= thresh.vals[y]))),
         samp.p = signif(samp.n/samp.size, 3)) -> tru.cdfs

tru.cdfs %>% 
  knitr::kable()


```


```{r LOMAX}
# All the lomax functions

# FITTING -------------------------------------
# We use a Lomax distribution, which is actually a special case of generalized Pareto distribution
# The Lomax or Patro type II has support on x=0, and is a heavy tail distribution
# Jose's document shows that it arises from incorporating variability in the rate parameter of an exponential distribution, using a gamma distribution - which he calls the hierarchical
# It only has two parameters, alpha(shape) and lambda(scale)

lomax.pdf <- function(x,alpha,k, log.scale=FALSE){
  
  if(log.scale==FALSE){out <- (k/(alpha+x))*(alpha/(alpha+x))^k
  }else{
    out <- log(k) + k*log(alpha) - (k+1)*log(alpha+x)
  }
  
  return(out)
}


lomax.cdf <- function(x,alpha,k){
  
  return(1-(alpha/(alpha+x))^k)
  
}

lomax.st <- function(x=x, alpha=alpha, k=k){
  out <- alpha/(alpha+x)
  return(out^k)
}

lomax.mean <- function(alpha=alpha, k=k){
  return(alpha/(k+1))
}


ft.nllike2 <- function(guess=init.betas, designmat=designmat,Y=Y){
  
  nbetasp1      <- length(init.betas)
  k             <- exp(guess[1])
  Xbeta         <- designmat%*%guess[2:nbetasp1]
  alphas        <- exp(Xbeta) # because alpha = ln(X*betas)
  n             <- length(Y)
  
  #sumlogapy     <- sum(log(alphas+Y))
  #k.hat         <- n/(sumlogapy - sum(Xbeta))
  lnft.yis     <- lomax.pdf(x=Y, alpha=alphas,k=k,log.scale=TRUE)
  lnL           <- sum(lnft.yis)
  nll           <- -lnL
  
  return(nll)
}


lomax.glm <- function(formula=~1, my.dataf, response){
  
  Y           <- response
  n           <- length(Y)
  designmat   <- model.matrix(formula, data=my.dataf)
  nbetas      <- ncol(designmat)
  init.betas  <- c(4,rep(5,nbetas))
  
  opt.out <- optim(par=init.betas, fn=ft.nllike2, method = "Nelder-Mead",
                   designmat=designmat, Y=Y)
  
  mles          <- opt.out$par
  nll.hat       <- opt.out$value
  BIC.mod       <- 2*nll.hat + nbetas*log(length(Y))
  Xbeta.hat     <- designmat%*%mles[-1]
  alphas.hat    <- exp(Xbeta.hat)
  #sumlogapy.hat <- sum(log(alphas.hat+Y))
  k.hat         <-  mles[1] #n/(sumlogapy.hat - sum(Xbeta.hat))
  
  out.list <- list(opt.out = opt.out, designmat=designmat,Y=Y, mles=mles, nll.hat=nll.hat, BIC.mod = BIC.mod,
                   alphas.hat=alphas.hat, k.hat=k.hat,data=my.dataf)
  
  return(out.list)
  
}

```

Running the experiment once:  

```{r}
# First estimate the mles for the different sample sizes

mles.sampsizes <- data.frame(samp.size = samp.sizes, alphas = 0, khats = 0)

for(i in 1:length(samp.sizes)){
  ith.n   <- mles.sampsizes$samp.size[i]
  ith.samples <- data.frame(x = sample(simplsamps$data, ith.n))
  
  # The glm.lomax was giving me an error. Kept saying it couldn't find the init betas but worked when I ran it line by line. 
  
  # From the glm framework, we are not including covariates here
  # The next three lines are the arguments to the glm lomax function
  formula=~1
  my.dataf = ith.samples
  response = ith.samples$x
  
  # This is the glm lomax function deconstructed
  Y           <- response
  n           <- length(Y)
  designmat   <- model.matrix(formula, data=my.dataf)
  nbetas      <- ncol(designmat)
  init.betas  <- c(4,rep(5,nbetas))
  
  opt.out <- optim(par=init.betas, fn=ft.nllike2, method = "Nelder-Mead",
                   designmat=designmat, Y=Y)
  
  mles          <- opt.out$par
  nll.hat       <- opt.out$value
  BIC.mod       <- 2*nll.hat + nbetas*log(length(Y))
  Xbeta.hat     <- designmat%*%mles[-1]
  alphas.hat    <- exp(Xbeta.hat)
  #sumlogapy.hat <- sum(log(alphas.hat+Y))
  k.hat         <-  mles[1] #n/(sumlogapy.hat - sum(Xbeta.hat))
  
  mles.sampsizes$alphas[i] <- alphas.hat
  mles.sampsizes$khats[i] <- k.hat
}

mles.sampsizes %>% 
  mutate(mean = lomax.mean(alphas, khats),
         tail100 = lomax.cdf(100, alphas, khats),
         tail250 = lomax.cdf(250, alphas, khats),
         tail500 = lomax.cdf(500, alphas, khats),
         tail750 = lomax.cdf(750, alphas, khats),
         tail1k = lomax.cdf(1000, alphas, khats)) %>% 
  #mutate(across(c(2:8), signif, digits=3)) %>% 
  mutate(across(starts_with("tail"), signif, digits=3))
  

```
```{r}
# But if we do 1-cdf, which should be P(X>=x) we get high numbers. 

1-lomax.cdf(thresh.vals, alpha = mles.sampsizes$alphas[1], k = mles.sampsizes$khats[1])

```
```{r eval=FALSE, echo=FALSE}
###################################
# Run the experiments with different sample sizes and tail thresholds

all.samples <- simplsamps$data

samp.sizes <- c(80, 200, 500, 800, 1000, 1600)
num.ns <- length(samp.sizes)
q.tests <- c(100, 250,500,1000)
num.qs <- length(q.tests)

all.sampsizes <- rep(samp.sizes,each=num.qs)
all.qtests <- rep(q.tests,num.ns)
ntests <- length(all.sampsizes)

cdfs.hat <- rep(0, ntests)


for(i in 1:ntests){
  ith.n   <- all.sampsizes[i]
  ith.samples <- data.frame(x = sample(all.samples, ith.n))
  
  formula=~1
  my.dataf = ith.samples
  response = ith.samples$x
  
  
  Y           <- response
  n           <- length(Y)
  designmat   <- model.matrix(formula, data=my.dataf)
  nbetas      <- ncol(designmat)
  init.betas  <- c(4,rep(5,nbetas))
  
  opt.out <- optim(par=init.betas, fn=ft.nllike2, method = "Nelder-Mead",
                   designmat=designmat, Y=Y)
  
  mles          <- opt.out$par
  nll.hat       <- opt.out$value
  BIC.mod       <- 2*nll.hat + nbetas*log(length(Y))
  Xbeta.hat     <- designmat%*%mles[-1]
  alphas.hat    <- exp(Xbeta.hat)
  #sumlogapy.hat <- sum(log(alphas.hat+Y))
  k.hat         <-  mles[1] #n/(sumlogapy.hat - sum(Xbeta.hat))
  
  mod1 <- list(opt.out = opt.out, designmat=designmat,Y=Y, mles=mles, nll.hat=nll.hat, BIC.mod = BIC.mod,
                   alphas.hat=alphas.hat, k.hat=k.hat,data=my.dataf)
  

  ith.a <- mod1$alphas.hat[1]
  ith.k <- mod1$k.hat

  ith.q <- all.qtests[i]
  ith.cdf <- 1- lomax.cdf(x = ith.q, alpha = ith.a, k = ith.k)
  cdfs.hat[i] <- ith.cdf

}

```




# GP approach  

Going off script here and going to try and do this in a way I understand it and with functions I already know. 

```{r results='hide'}
library(extRemes)

fevd.mles <- data.frame(samp.size = rep(samp.sizes, length(thresh.vals)), threshold = thresh.vals, scale = 0, shape = 0, nllh = 0)

for(i in 1:nrow(fevd.mles)){
  ith.n        <- fevd.mles$samp.size[i]
  ith.samples  <- data.frame(x = sample(simplsamps$data, ith.n))
  
  # So, setting the threshold to 0 so we can compare to Lomax.
  
  
  ith.fit      <- fevd(ith.samples$x, threshold = 0, type = "GP") 
  
  mles          <- summary(ith.fit)$par
  nll.hat       <- summary(ith.fit)$nllh
  BIC.mod       <- summary(ith.fit)$BIC
  
  fevd.mles$scale[i] <- mles[1]
  fevd.mles$shape[i] <- mles[2]
  fevd.mles$nllh[i]  <- nll.hat
  
  ith.thresh   <- fevd.mles$threshold[i]
  fevd.mles$gp.tail[i] <- pextRemes(ith.fit, ith.thresh, lower.tail = FALSE)
}

```



```{r}
fevd.mles %>% 
  knitr::kable()
```


So, we try to make the link to a Generalized Pareto distribution with the Lomax.The Lomax distribution is a special case of a Pareto where the parameters are as follows:  
- The location is $\mu = 0$ 
- The shape parameter is $\xi = 1/\alpha$
- The scale parameter is $\sigma = \lambda/\alpha$  

Where the pdf of the lomax follows:
$$
p(x) = \frac{\alpha}{\lambda}\left[ 1 +  \frac{x}{\lambda}  \right]^{-\alpha + 1} = \left( \frac{\lambda}{x+\lambda} \right)^{\alpha} \left( \frac{\alpha}{x+\lambda} \right)
$$
And in JM Ponciano's parameterization: $\alpha = k$ and $\lambda = \alpha$, which means in comparison to the Generalized Pareto:  

- The location is $\mu = 0$ 
- The shape parameter is $\xi = 1/k$
- The scale parameter is $\sigma = \alpha/k$  


```{r}
gp2lomax.jmp <- function(shape, scale){
  k     <- 1/shape
  alpha <- scale*k
}
```

So, now we use the parameter estimates from the GP to get the parameters as a Lomax, and use those parameters to get the cdfs and means.

```{r}

fevd.mles %>% 
  mutate(ks = 1/shape,
         alphas = scale*ks) %>% 
  #dplyr::select(-c(scale, shape)) %>% 
  mutate(est.cdf = lomax.cdf(threshold, alphas, ks),
         tail = 1- est.cdf, 
         mean = lomax.mean(alphas, ks)) -> est.lomax.mles
est.lomax.mles %>% knitr::kable()

```  
The NaN I get in the previous dataframe are due to 0/0, so, undefined? What should I make of it?

So, now I will estimate the ratio between the estimated cdf and the true cdf.

```{r fig.width=8}


est.lomax.mles %>% 
  mutate(tru.tails = rep(tru.cdfs$w.cdfs, length(samp.sizes)),
         tails.ratio = tail/tru.tails,
         thresh = factor(threshold)) %>%
  drop_na(tails.ratio) %>% 
  mutate(nice.tails = signif(tails.ratio, 3)) %>% 
  ggplot(., aes(x = samp.size, y = tails.ratio, color = thresh)) +
  geom_point() +
  labs(title = "Lomax") +
  theme_minimal() -> lomax.tail


est.lomax.mles %>% 
  mutate(tru.tails = rep(tru.cdfs$w.cdfs, length(samp.sizes)),
         gp.tails.ratio = gp.tail/tru.tails,
         thresh = factor(threshold)) %>%
  mutate(nice.tails = signif(gp.tails.ratio, 3)) %>% 
  ggplot(., aes(x = samp.size, y = gp.tails.ratio, color = thresh)) +
  geom_point() +
  labs(title = "GP") +
  theme_minimal() -> gp.tail

plot_grid(lomax.tail, gp.tail)


```


Now, we repeat the process to see how much this varies...  


```{r results='hide'}
nreps <- 1000
gp.mles.reps <- data.frame(NULL)

for(j in 1:nreps){
  
  ith.mle.df <- data.frame(samp.size = rep(samp.sizes, length(thresh.vals)), threshold = thresh.vals,
                           scale = 0, shape = 0, nllh = 0, gp.tail = 0, rep = 0)
  
  for(i in 1:nrow(ith.mle.df)){
  ith.n        <- ith.mle.df$samp.size[i]
  ith.samples  <- data.frame(x = sample(simplsamps$data, ith.n))
  
  # So, setting the threshold to 0 so we can compare to Lomax.

  
  ith.fit      <- fevd(ith.samples$x, threshold = 0, type = "GP") 
  
  mles          <- summary(ith.fit)$par
  nll.hat       <- summary(ith.fit)$nllh
  BIC.mod       <- summary(ith.fit)$BIC
  
  ith.mle.df$scale[i] <- mles[1]
  ith.mle.df$shape[i] <- mles[2]
  ith.mle.df$nllh[i]  <- nll.hat
  ith.mle.df$rep[i] <- paste0("rep", j)
  
  ith.thresh   <- ith.mle.df$threshold[i]
  ith.mle.df$gp.tail[i] <- pextRemes(ith.fit, ith.thresh, lower.tail = FALSE)
  
  }
  
  gp.mles.reps <- rbind.data.frame(gp.mles.reps, ith.mle.df)
  
}



gp.mles.reps %>% 
  mutate(ks = 1/shape,
         alphas = scale*ks) %>% 
  #dplyr::select(-c(scale, shape)) %>% 
  mutate(est.cdf = lomax.cdf(threshold, alphas, ks),
         tail = 1- est.cdf, 
         mean = lomax.mean(alphas, ks)) -> gp.mles.reps

save(gp.mles.reps, file = "Ch3_samplesize/testdf.RData")
```  

Let's see


```{r fig.width=8}

n.tru.cdfs <- nrow(gp.mles.reps)/length(tru.cdfs$w.cdfs)

gp.mles.reps %>% 
  mutate(tru.tails = rep(tru.cdfs$w.cdfs, n.tru.cdfs),
         tails.ratio = tail/tru.tails,
         thresh = factor(threshold),
         samp.size = factor(samp.size)) %>%
  drop_na(tails.ratio) %>% 
  mutate(nice.tails = signif(tails.ratio, 3)) %>% 
  ggplot(., aes(x = samp.size, y = tails.ratio, color = thresh)) +
  geom_boxplot() +
  labs(title = "Lomax") +
  theme_minimal() -> lomax.tail


gp.mles.reps %>% 
  mutate(tru.tails = rep(tru.cdfs$w.cdfs, n.tru.cdfs),
         gp.tails.ratio = gp.tail/tru.tails,
         thresh = factor(threshold),
         samp.size = factor(samp.size)) %>%
  mutate(nice.tails = signif(gp.tails.ratio, 3)) %>% 
  ggplot(., aes(x = samp.size, y = gp.tails.ratio, color = thresh)) +
  geom_boxplot() +
  labs(title = "GP") +
  theme_minimal() -> gp.tail

plot_grid(lomax.tail, gp.tail)

```


Restrict the y axis  

```{r fig.width=8}

n.tru.cdfs <- nrow(gp.mles.reps)/length(tru.cdfs$w.cdfs)

gp.mles.reps %>% 
  mutate(tru.tails = rep(tru.cdfs$w.cdfs, n.tru.cdfs),
         tails.ratio = tail/tru.tails,
         thresh = factor(threshold),
         samp.size = factor(samp.size)) %>%
  drop_na(tails.ratio) %>% 
  mutate(nice.tails = signif(tails.ratio, 3)) %>% 
  ggplot(., aes(x = samp.size, y = tails.ratio, color = thresh)) +
  geom_boxplot() +
  labs(title = "Lomax") +
  lims(y = c(-1, 3)) +
  theme_minimal() -> lomax.tail


gp.mles.reps %>% 
  mutate(tru.tails = rep(tru.cdfs$w.cdfs, n.tru.cdfs),
         gp.tails.ratio = gp.tail/tru.tails,
         thresh = factor(threshold),
         samp.size = factor(samp.size)) %>%
  mutate(nice.tails = signif(gp.tails.ratio, 3)) %>% 
  ggplot(., aes(x = samp.size, y = gp.tails.ratio, color = thresh)) +
  geom_boxplot() +
  labs(title = "GP") +
  lims(y = c(-1, 3)) +
  theme_minimal() -> gp.tail

plot_grid(lomax.tail, gp.tail)

```

Check the mean:  

```{r}
gp.mles.reps %>% 
  mutate(tru.tails = rep(tru.cdfs$w.cdfs, n.tru.cdfs),
         tails.ratio = tail/tru.tails,
         gp.tails.ratio = gp.tail/tru.tails) %>% 
  group_by(samp.size, threshold) %>% 
  summarise(mean.ratio = mean(tails.ratio),
            mean.gp.ratio = mean(gp.tails.ratio))
```

